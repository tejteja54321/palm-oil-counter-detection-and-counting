{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c1af0c-f369-49f3-9e35-445c7d77531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Main Python2\\anaconda3\\envs\\proj10\\lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b413587-4106-4b54-99c6-a97b9f18c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OilPalmDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx].replace('.jpg', '.txt'))\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        boxes = np.loadtxt(label_path).reshape(-1, 5)  # Format: [class, x, y, w, h]\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=boxes[:, 1:], labels=boxes[:, 0])\n",
    "            image = transformed['image']\n",
    "            boxes = torch.tensor(transformed['bboxes'])\n",
    "            labels = torch.tensor(transformed['labels'])\n",
    "        \n",
    "        return image, boxes, labels\n",
    "\n",
    "# Define Transformations\n",
    "transform = A.Compose([\n",
    "    A.Resize(640, 640),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['labels']))\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = OilPalmDataset('F:/TEJA/IP/PROJ_10/train/images', 'F:/TEJA/IP/PROJ_10/train/labels', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "557e03a3-64d3-4bdd-858f-c4c136822191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define YOLOv10 Model\n",
    "class YOLOv10(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(YOLOv10, self).__init__()\n",
    "        \n",
    "        # Backbone: Convolutional layers\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Transformer-based Feature Fusion\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=128, nhead=8)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # Detection Head\n",
    "        self.detect_head = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(),\n",
    "            nn.Conv2d(256, num_classes + 4, 1)  # [class probabilities + 4 box coordinates]\n",
    "        )\n",
    "        \n",
    "        # Object Counting Head\n",
    "        self.counting_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 40 * 40, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)  # Single regression value for counting\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1).permute(2, 0, 1)  # Reshape for transformer\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 2, 0).view(x.size(1), 128, 40, 40)  # Reshape back\n",
    "        \n",
    "        detections = self.detect_head(x)\n",
    "        counting = self.counting_head(x)\n",
    "        return detections, counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f69925-04fc-45d8-9df1-16938d56a788",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TransformerEncoder' from 'transformers' (C:\\Users\\Main Python2\\anaconda3\\envs\\proj10\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerEncoder, TransformerEncoderLayer\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLOv10(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TransformerEncoder' from 'transformers' (C:\\Users\\Main Python2\\anaconda3\\envs\\proj10\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YOLOv10(num_classes=5).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.MSELoss()  # Use for counting\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(dataloader, leave=True)\n",
    "        for images, _, _ in loop:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            detections, counting = model(images)\n",
    "            target_counts = torch.tensor([len(_) for _ in _]).float().to(device)\n",
    "            loss = criterion(counting.squeeze(), target_counts)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loop.set_description(f\"Epoch [{epoch}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "train(model, train_loader, optimizer, criterion, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553c2c6-8b36-4eed-a34b-564e06d46652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
